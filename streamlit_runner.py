"""
Streamlit åº”ç”¨å¯åŠ¨å™¨
ç”¨äºå¤„ç†å¼‚æ­¥å·¥ä½œæµä¸ Streamlit çš„é›†æˆ
"""

import asyncio
import streamlit as st
from typing import Dict, Any
import time
import threading
from concurrent.futures import ThreadPoolExecutor

# å¯¼å…¥æ·±åº¦ç ”ç©¶æ¨¡å—
try:
    from deep_research import graph

    GRAPH_AVAILABLE = True
except ImportError as e:
    st.error(f"æ— æ³•å¯¼å…¥æ·±åº¦ç ”ç©¶æ¨¡å—: {e}")
    GRAPH_AVAILABLE = False


class StreamlitAsyncRunner:
    """å¤„ç† Streamlit ä¸­çš„å¼‚æ­¥æ“ä½œ"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=2)

    def run_async_in_thread(self, coro, callback=None):
        """åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°"""

        def thread_target():
            try:
                # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                result = loop.run_until_complete(coro)
                loop.close()
                if callback:
                    callback(result)
                return result
            except Exception as e:
                if callback:
                    callback({"error": str(e)})
                return {"error": str(e)}

        future = self.executor.submit(thread_target)
        return future


# å…¨å±€å¼‚æ­¥è¿è¡Œå™¨
async_runner = StreamlitAsyncRunner()


async def run_research_workflow(topic: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """è¿è¡ŒçœŸå®çš„æ·±åº¦ç ”ç©¶å·¥ä½œæµ"""
    if not GRAPH_AVAILABLE:
        return {
            "final_report": f"# {topic} - æ¨¡æ‹Ÿç ”ç©¶æŠ¥å‘Š\n\nç”±äºæ·±åº¦ç ”ç©¶æ¨¡å—æœªæ­£ç¡®åŠ è½½ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹ŸæŠ¥å‘Šã€‚",
            "error": "æ·±åº¦ç ”ç©¶æ¨¡å—ä¸å¯ç”¨",
        }

    try:
        # è¿è¡Œå®é™…çš„ LangGraph å·¥ä½œæµ
        result = await graph.ainvoke({"topic": topic}, config=config)
        return result
    except Exception as e:
        return {"error": str(e), "final_report": f"ç ”ç©¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"}


def create_research_config(
    writer_model: str, planner_model: str, search_depth: int
) -> Dict[str, Any]:
    """åˆ›å»ºç ”ç©¶é…ç½®"""
    return {
        "configurable": {
            "thread_id": f"streamlit_{int(time.time())}",
            "max_search_depth": search_depth,
            "writer_model": writer_model,
            "planner_model": planner_model,
            "writer_provider": "openai",  # é»˜è®¤æä¾›å•†
            "planner_provider": "anthropic",  # é»˜è®¤æä¾›å•†
        }
    }


def format_research_result(result: Dict[str, Any], topic: str) -> tuple:
    """æ ¼å¼åŒ–ç ”ç©¶ç»“æœ"""
    if "error" in result:
        return f"âŒ ç ”ç©¶å¤±è´¥: {result['error']}", {}

    final_report = result.get("final_report", "æœªç”ŸæˆæŠ¥å‘Š")

    # æå–å…ƒæ•°æ®
    metadata = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "topic": topic,
        "sections_count": final_report.count("##") if final_report else 0,
        "word_count": len(final_report.split()) if final_report else 0,
    }

    if "sections" in result:
        metadata["sections"] = [s.name for s in result["sections"]]

    if "source_str" in result:
        metadata["sources_used"] = True

    return final_report, metadata


# æµ‹è¯•å‡½æ•°
def test_streamlit_integration():
    """æµ‹è¯• Streamlit é›†æˆ"""
    st.write("ğŸ§ª æµ‹è¯•æ·±åº¦ç ”ç©¶æ¨¡å—é›†æˆ...")

    if GRAPH_AVAILABLE:
        st.success("âœ… æ·±åº¦ç ”ç©¶æ¨¡å—åŠ è½½æˆåŠŸ")
        st.write("ğŸ“Š å·¥ä½œæµçŠ¶æ€: å¯ç”¨")
    else:
        st.warning("âš ï¸ æ·±åº¦ç ”ç©¶æ¨¡å—æœªæ­£ç¡®åŠ è½½ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    with st.expander("ğŸ”§ é…ç½®è¯¦æƒ…"):
        st.json(
            {
                "graph_available": GRAPH_AVAILABLE,
                "async_runner": "å·²åˆå§‹åŒ–",
                "thread_pool": "å¯ç”¨",
            }
        )


if __name__ == "__main__":
    # å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œæ˜¾ç¤ºæµ‹è¯•ä¿¡æ¯
    st.title("ğŸ”¬ Streamlit å¼‚æ­¥è¿è¡Œå™¨æµ‹è¯•")
    test_streamlit_integration()
